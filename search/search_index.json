{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to UnBIAS","text":""},{"location":"#overview","title":"Overview","text":"<p>PyPI: https://pypi.org/project/UnBIAS/</p> <p>UnBIAS is a is a state-of-the-art text analysis and debiasing toolkit that aids in assessing and rectifying biases in textual content. Developed with state-of-the-art Transformer models, this toolkit offers:</p> <ul> <li> <p>Bias Classification: Evaluate textual content and classify its level of bias.</p> </li> <li> <p>Named Entity Recognition (NER) for Bias: Detect specific terms or entities in the text which may hold biased sentiments.</p> </li> <li> <p>Text Debiasing: Process any text and receive a debiased version in return. This ensures the content is neutral concerning gender, race, age groups, and is free from toxic or harmful language.</p> </li> </ul> <p>Our models are built on BERT, RobERTa and Meta - LLama-2-7B quantized models.</p> <p></p>"},{"location":"#how-to-install-unbias","title":"How to install UnBIAS","text":"<p>To use UnBIAS, you'll need to have Python installed on your system. UnBIAS supports Python3 and above.</p>"},{"location":"#installation-via-pip","title":"Installation via pip","text":"<p>The recommended way to install UnBIAS is via pip, the Python package manager. Open your terminal or command prompt and run the following command:</p> <pre><code>pip install UnBIAS\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":"<p>The UnBIAS library provides a function: <code>run_pipeline_on_texts</code> which will perform bias classification, NER for bias, and generate a debiased version of your text. A simple usage of the library is as follows: </p> run_unbias_simple.py<pre><code>from UnBIAS import run_pipeline_on_texts\n\n# Define your test sentences\nbiased_texts =  [\"Men make better programmers than woman\", \\\n                \"People who wear Y clothing are untrustworthy.\"]\n# Run the pipeline on the text. This will return a pandas dataframe with columns:\n# [Original_Text, Bias_Classification, NER_Bias, Debiased_Text]\nresults = run_pipeline_on_texts(biased_texts)\n# Save the results to a csv file\nresults.to_csv('&lt;save_path&gt;.csv', index=False)\n</code></pre> <p>If you have a dataset CSV file, you can extract the text column and run the pipeline on it as follows:</p> run_unbias_on_dataset.py<pre><code>from UnBIAS import run_pipeline_on_texts\nimport pandas as pd \n\nbiased_texts = pd.read_csv(\"&lt;dataset_path&gt;.csv\")\n# Assuming your CSV has a column called 'Text' that you want to debias\nbiased_texts = list(biased_texts['Text'])\n# run the pipeline and save results\nresults = run_pipeline_on_texts(biased_texts)\nresults.to_csv('&lt;save_path&gt;.csv', index=False)\n</code></pre> <p>Refer to the API Reference for more information about <code>run_pipeline_on_texts</code>.</p>"},{"location":"#additional-highlights","title":"Additional Highlights","text":"<ul> <li> <p>Pre-trained Models: Uses specialized models from the renowned Hugging Face's Transformers library. We have also tailored Transformer-based models for bias detection and debiasing tasks newsmediabias-hub.</p> </li> <li> <p>Efficient Pipelines: Designed with intuitive pipelines, making it easier to incorporate into applications or other projects.</p> </li> <li> <p>Analytical Tools: Handy tools available to transform results into structured data for further analysis.</p> </li> </ul>"},{"location":"#what-to-do-next","title":"What to do next?","text":"<ul> <li>You can individually run each stage of the pipeline. For instance, maybe you only care about debiasing text without concern for classification or NER for bias. To see how to do this, refer to the BiasPipeline class in the API.</li> <li>Debiasing text works by prompting the Llama2 LLM with a specific prompt. You can modify this prompt to suit your needs. See the Constants section for more information.</li> </ul>"},{"location":"#project-information","title":"Project Information","text":"<ul> <li>License: MIT</li> <li>PyPI: https://pypi.org/project/UnBIAS/</li> <li>Source Code: https://github.com/VectorInstitute/NewsMediaBias OR ask email: Shaina Raza</li> </ul>"},{"location":"#contact-me","title":"Contact Me","text":"<p>Shaina Raza, PhD Applied Machine Learning Scientist - Responsible AI Vector Institute for Artificial Intelligence  </p> <p>Email: Shaina.raza@utoronto.ca</p> <p>A special thanks to Oluwanifemi Bamgbose, Brandon Jaipersaud, Shardul Ghuge and Tahniat Khan who helped in the development of this project.</p>"},{"location":"Usecases/","title":"Use Cases","text":""},{"location":"Usecases/#use-cases-and-addressing-biases-with-unbias","title":"Use Cases and Addressing Biases with UnBIAS","text":""},{"location":"Usecases/#1-news-article-analysis","title":"1. News Article Analysis","text":"<p>Scenario: News agencies experience shifts in global perspectives, necessitating adjustments in how biases are detected in new articles.</p> <p>Steps:  - Continually gather labeled data from newer articles. - Periodically re-train the UnBIAS model with the updated dataset to reflect evolving biases and narratives. - Integrate the latest model to monitor and rectify biases in published content.</p>"},{"location":"Usecases/#2-business-communications","title":"2. Business Communications","text":"<p>Scenario: A multinational corporation expands to new regions, encountering diverse cultural and linguistic nuances.</p> <p>Steps:  - Source region-specific data that highlights potential biases unique to the new location. - Augment the training dataset with this data and re-train the UnBIAS model. - Incorporate the updated model in the communication approval process to ensure local sensitivities are respected.</p>"},{"location":"Usecases/#3-social-media-monitoring","title":"3. Social Media Monitoring","text":"<p>Scenario: A brand launches a new product, leading to different kinds of discussions and potentially new biases on social media.</p> <p>Steps:  - Extract and label relevant discussions about the new product. - Update the UnBIAS model by training it with the augmented dataset. - Use the refined model for real-time analysis of brand mentions and address biases appropriately.</p>"},{"location":"Usecases/#4-financial-analysis","title":"4. Financial Analysis","text":"<p>Scenario: A financial institution uses UnBIAS to monitor biases in investor communications. Over time, market dynamics change, leading to new terminologies and potential biases.</p> <p>Steps:  - Gather investor communications, especially those around new market phenomena or products. - Label the dataset to identify new forms of biases tied to the evolving financial landscape. - Re-train the UnBIAS model with this data to keep the bias detection updated. - Integrate the model into the institution's communication analysis pipeline to ensure accurate bias detection.</p> <p>Note:</p> <p>Re-training models is essential when dealing with evolving domains. As language and societal perspectives change, models like UnBIAS should be updated to maintain their accuracy and relevance. Periodic re-training ensures that the tool remains effective in various scenarios, including specialized domains like finance. See the Data Preparation section for re-training the pipeline.</p>"},{"location":"bias_pipeline/","title":"BiasPipeline: Class","text":"<p>This class can be used to individually run the bias classification and NER for bias stages of the pipeline. See the example code snippets below on how to do this. It is also used by the <code>run_pipeline_on_texts</code> function to perform the entire pipeline on a list of texts. </p> <p>To individually run the debiasing stage of the pipeline see the <code>get_debiased_sequence</code> function.</p>"},{"location":"bias_pipeline/#__init__-none","title":"<code>__init__() -&gt; None</code>","text":"<p>Called upon initialization. It calls <code>load_resources()</code> below.</p>"},{"location":"bias_pipeline/#load_resources-none","title":"<code>load_resources() -&gt; None</code>","text":"<p>Loads the bias classifier, tokenizers and models.</p>"},{"location":"bias_pipeline/#predict_entitiestextstr-liststr","title":"<code>predict_entities(text:str) -&gt; list[str]</code>","text":"<p>Retrieves biased entities from a text. </p>  Parameters  text: The text to retrieve biased entities from.  Returns  A list of biased entities in the text. <p> Example </p> <pre><code>import UnBIAS.unbias as unbias\n\nsentence = \"\"\"Last Tuesday, Elon Musk announced a new project on Mars, \n            which would be funded with $6 billion from Tesla, and he discussed  \n            this in New York during a United Nations meeting.\"\"\"\n\nbias_pipeline = unbias.BiasPipeline()\nbiased_entities = bias_pipeline.predict_entities(sentence)\nprint(biased_entities)\n</code></pre> <p> Output  <code>['new-B-BIAS', 'project-I-BIAS', 'funded-B-BIAS', 'discussed-B-BIAS', 'this-I-BIAS', 'new-B-BIAS', 'united-B-BIAS', 'nations-I-BIAS', 'meeting-I-BIAS']</code></p> <p>B-BIAS stands for beginning of entity and I-BIAS stands for inside of entity. This is because a biased entity can span multiple words.</p> <p>Warning</p> <p>If running the above multiple times in a Jupyter Notebook, you might have to do <code>del bias_pipeline</code> before running it again to avoid out-of-memory errors.</p>"},{"location":"bias_pipeline/#predict_biastextsstrliststr-listdictstr-str-str-float","title":"<code>predict_bias(texts:str|list[str]) -&gt; list[dict[str: str, str: float]]</code>","text":"<p>Performs bias classification on a text or a list of texts. </p>  Parameters  texts: The text or list of texts to classify.  Returns  An array of dictionaries where each dictionary is of the form:  <code>{'label': Non-biased|Biased, 'score': &lt;bias_score&gt;}</code>.  <p> Example </p> <pre><code>import UnBIAS.unbias as unbias\n\nsentences = [\"The weather is cold outside.\", \"Men are dumb.\"]\nbias_pipeline = unbias.BiasPipeline()\nsentence_classifications = bias_pipeline.predict_bias(sentences)\nprint(sentence_classifications)\n</code></pre> <p> Output  <code>[{'label': 'Non-biased', 'score': 0.9980363249778748}, {'label': 'Biased', 'score': 0.9899910688400269}]</code></p>"},{"location":"constants/","title":"Constants","text":""},{"location":"constants/#sys_messagestr","title":"<code>sys_message:str</code>","text":"<ul> <li>default value: Task: Please just generate a bias-free version of the text provided, ensuring it's free from biases related to  age, gender, politics, social nuances, or economic background, while keeping it roughly the same length as the original:</li> </ul>"},{"location":"constants/#instructionstr","title":"<code>instruction:str</code>","text":"<ul> <li>default value: Instruction: As a helpful, respectful and trustworthy debiasing assistant, your task is to receive a text and return its unbiased version, Don't add additional comment. Just return the  un biased version of the input text:</li> </ul>  Modification  <p>The above two parameters are used to prompt Llama2 to debias text. Feel free to change these constants to suit your task. You can even play around with various debiasing prompts to see if it performs better than what we currently use for debiasing.</p> <p>Here is an example of modifying the constants for the summarization of complex information:</p> <pre><code>import UnBIAS.unbias as unbias\n\nunbias.sys_message = \"Task: Please generate a simplified version of the provided text, ensuring it's easy to understand, devoid of technical jargon, and accessible to readers of all education levels, while keeping it concise and retaining the original's core information:\"\nunbias.instruction = \"Instruction: As a friendly, approachable, and reliable simplification assistant, your task is to receive a text and return its simplified version. Don't add personal opinions or extraneous explanations. Just return the text in a form that's easier to understand for the average reader:\"\n\nlong_text = \"In the field of quantum computing, the Planck scale (named after Max Planck), is the scale of energy (approximately 1.22 x 10^19 GeV) at which quantum effects of gravity become strong. At this scale, it is believed that the effects of quantum mechanics and general relativity converge, necessitating a unified framework, commonly referred to as quantum gravity, to adequately describe phenomena. However, achieving a comprehensive theory of quantum gravity has remained elusive, and it presents one of the most significant challenges in theoretical physics.\"\n\nsimplified_text = unbias.get_debiased_sequence(long_text)\nprint(simplified_text)\n</code></pre>  Output  <p><code>In quantum computing, the Planck scale (named after Max Planck), marks the point where the effects of gravity become significant and require a unified framework to describe phenomena.</code></p>"},{"location":"core_functions/","title":"Core Functions","text":""},{"location":"core_functions/#get_debiased_sequencetextstr-str","title":"<code>get_debiased_sequence(text:str) -&gt; str</code>","text":"<p>Debiases the given text by prompting Llama2.</p>  Parameters  text: The text to debias.  Returns  The debiased text. <p> Example </p> <pre><code>import UnBIAS.unbias as unbias\n\ndebiased_sequence = unbias.get_debiased_sequence(\"Men are dumb.\")\nprint(debiased_sequence)\n</code></pre> <p> Output  <code>I apologize, but it's important to avoid making gender-based generalizations or insults. Men and women are individuals with their own unique perspectives and experiences.</code></p> <p>Note</p> <p>The output of debiasing this sentence can vary since Llama2 is a probabilistic model. This is why it varies from when we debiased it in the <code>create_bias_analysis_dataframe</code> example below.</p>"},{"location":"core_functions/#create_bias_analysis_dataframetextsliststr-pipeline_instancebiaspipeline-pddataframe","title":"<code>create_bias_analysis_dataframe(texts:list[str], pipeline_instance:BiasPipeline) -&gt; pd.DataFrame</code>","text":"<p>Runs the pipeline on each text in the list and returns a dataframe with the results.</p>  Parameters  texts: The text to debias. pipeline_instance: A BiasPipeline instance used to run the pipeline.  Returns  A Pandas DataFrame with columns: [Original_Text, Label_Bias, Biased_Phrases, Debiased_Text]. <p> Example </p> <p><pre><code>import UnBIAS.unbias as unbias\n\n# Define your test sentences\nbiased_texts =  [\"The weather is cold outside.\", \"Men are dumb.\"]\n\n# Use the function. Gets saved to debiased_results.csv\nresults = unbias.run_pipeline_on_texts(biased_text)\nprint(results)\n</code></pre>  Output </p> Original_Text Label_Bias Biased_Phrases Debiased_Text The weather is cold outside. NON-BIASED(99%) [cold-B-BIAS, outside-I-BIAS] The temperature is currently cold. Men are dumb. BIASED(98%) [men-B-BIAS, are-I-BIAS, dumb-I-BIAS] Men may have different cognitive abilities or understanding perspectives. It is not accurate or fair to make broad generalizations about any particular group of people."},{"location":"core_functions/#run_pipeline_on_textstextsliststr-pddataframe","title":"<code>run_pipeline_on_texts(texts:list[str]) -&gt; pd.DataFrame</code>","text":"Runs all three stages of the pipeline on the provided list of texts and saves the resulting DataFrame to a file named <code>debiased_results.csv</code>. <p>This function calls <code>create_bias_analysis_dataframe</code> to generate the DataFrame and then saves it to  <code>debiased_results.csv</code>.</p>  Parameters  texts: The list of texts to run the pipeline on.  Returns  A Pandas dataframe with columns: [Original_Text, Label_Bias, Biased_Phrases, Debiased_Text]. <p> Example </p> <p>You can use this function in the same way as <code>create_bias_analysis_dataframe()</code>. The only difference is the generated DataFrame gets saved to <code>debiased_results.csv</code>.</p> <p>Note</p> <p>This function will overwrite any existing file called <code>debiased_results.csv</code> in your working directory.</p>"},{"location":"data_prep/","title":"FAIR Principles for Data Management","text":"<p>Ensuring data is Findable, Accessible, Interoperable, and Reusable is essential for effective data management, especially in the age of big data and AI. This repository shows our approach to adopting the FAIR principles, with an emphasis on specific data formats and methodologies for training classifiers.</p>"},{"location":"data_prep/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Binary and Multi-label Classification</li> <li>CONLL BIO FORMAT</li> <li>Debiasing Training Data</li> <li>Active Learning and Labeling</li> </ul>"},{"location":"data_prep/#introduction","title":"Introduction","text":"<p>The FAIR principles guide our data management strategy, ensuring our datasets are:</p> <ul> <li> <p>Findable: Easily located and identified.</p> </li> <li> <p>Accessible: Available for retrieval without unnecessary barriers.</p> </li> <li> <p>Interoperable: Compatible with other data systems and platforms.</p> </li> <li> <p>Reusable: Ready for future applications and re-analysis.</p> </li> </ul>"},{"location":"data_prep/#binary-and-multi-label-classification","title":"Binary and Multi-label Classification","text":"<p>To train the UnBIAS classifier, data should be labeled either as \"biased\" or \"unbiased\". Multi-label classifications are also supported.</p> <p>Example: <pre><code>Sentence: \"Women can't drive well.\"\nLabel: \"biased\"\n</code></pre></p>"},{"location":"data_prep/#conll-bio-format","title":"CONLL BIO FORMAT","text":"<p>For training named entities, data should be in the CONLL BIO format. </p> <p>\"B-\" means the beginning of an entity.</p> <p>\"I-\" means inside or continuation of an entity.</p> <p>\"O\" means outside of any entity.</p> <p>Example:</p> <p>Sentence: \"He's surprisingly articulate for a young man from that neighborhood.\" BIO Format:  <pre><code>He's            O\nsurprisingly    O\narticulate      O\nfor             B-BIAS\na               I-BIAS\nyoung           I-BIAS\nman             I-BIAS\nfrom            I-BIAS\nthat            I-BIAS\nneighborhood    I-BIAS\n.               O\n</code></pre> In the CONLL BIO format, the annotations \"O\", \"B-BIAS\", and \"I-BIAS\" have specific meanings:</p> <p>O: Represents \"Outside\" and is used for words that are not part of any entity or specifically not part of the entity being tagged.</p> <p>B-BIAS: Represents \"Begin-BIAS\". It indicates the beginning of a bias entity (or any other entity you're tracking). The \"B-\" prefix is used to mark the start of an entity.</p> <p>I-BIAS: Represents \"Inside-BIAS\". It indicates a word that is inside or a continuation of a bias entity that started with a \"B-BIAS\" tag.</p>"},{"location":"data_prep/#debiasing-training-data","title":"Debiasing Training Data","text":"<p>For the debiaser in UnBIAS, the dataset should contain both the original \"biased_text\" and its \"debiased_version\".</p> <p>Example: <pre><code>Biased Text: \"Men are better leaders.\"\nDebiased Version: \"Leadership ability isn't gender-specific.\"\n</code></pre></p>"},{"location":"data_prep/#active-learning-and-labeling","title":"Active Learning and Labeling","text":"<p>Active learning is a crucial component of our data preparation methodology for UnBIAS. Here's our approach:</p> <ol> <li>Iterative Labeling: Begin by labeling a small subset of the data.</li> <li>Model Training: Train a preliminary model using this subset.</li> <li>Uncertainty Sampling: The model pinpoints data points of uncertainty.</li> <li>Human Intervention: Experts label these uncertain points.</li> <li>Model Refinement: The model is retrained with the new labeled data.</li> </ol> <p></p> <p>Note:</p> <p>The above instructions are for cases where you want to train UnBIAS models on your training data or specific use case.</p> <p>The Unbias package allows you to easily process a batch of sentences to detect and rectify biases. Here's how you can do it:</p>"},{"location":"data_prep/#running-the-unbias-package","title":"Running the Unbias Package","text":"<pre><code>from UnBIAS import run_pipeline_on_texts\n\n# Define your test sentences\ntest_sentences = [\n    \"Women are just too emotional to be leaders.\",\n    \"All young people are lazy.\",\n    \"Men are naturally better at sports.\"\n]\n\n# Use the function\nresults = run_pipeline_on_texts(test_sentences)\nresult_df.head()\nresult_df.to_csv('UnBIAS-results.csv')\n</code></pre> <p>Contributors are welcome to aid in the enhancement of our documentation and methodologies.</p>"},{"location":"maintainers/","title":"Maintainers","text":"<p>Shaina Raza, PhD Applied Machine Learning Scientist, Responsible AI, Vector Institute for Artificial Intelligence Email: Shaina.raza@utoronto.ca</p>"},{"location":"utils/","title":"Utility Functions","text":"<p>Note</p> <p>These are internal functions used by UnBIAS to help run various parts of the pipeline. You will likely not need to use these. </p>"},{"location":"utils/#re_incomplete_sentencetextstr-str","title":"<code>re_incomplete_sentence(text:str) -&gt; str</code>","text":"<p>Removes incomplete sentences from a text. This may occur when Llama2 stops generating text due to a maximum token generation limit. For instance, a generated sentence may look like: <code>The man was hungry. He went to</code>, in which case it would remove <code>He went to</code> from the text.</p>  Parameters  text: The text to clean.  Returns  The text with any incomplete sentences removed. <p> Example </p> <pre><code>import UnBIAS.unbias as unbias\n\ncleaned_sentence = unbias.re_incomplete_sentence('The man was hungry. He went')\nprint(cleaned_sentence)\n</code></pre> <p> Output  <code>The man was hungry.</code></p>"},{"location":"utils/#tokenize_for_predictiontextstr-tokenizerpretrainedtokenizer-listint-listint","title":"<code>tokenize_for_prediction(text:str, tokenizer:PreTrainedTokenizer) -&gt; list[int], list[int]</code>","text":"<p>Tokenizes the text and returns the corresponding <code>input_ids</code> and <code>attention_mask</code>.</p>  Parameters  text: The text to tokenize. tokenizer: A Hugging Face Tokenizer.  Returns  input_ids: List of integers representing tokenized text. attention_mask: A list with the same length as input_ids, consisting of 1s and 0s, where 1 indicates a real token, and 0 indicates a padding token. This tells the model which tokens should be attended to. <p> Example </p> <pre><code>import UnBIAS.unbias as unbias\nfrom transformers import AutoTokenizer\n\nmodel = \"newsmediabias/UnBIAS-LLama2-Debiaser-Chat-QLoRA\"\ntokenizer = AutoTokenizer.from_pretrained(model)\ntokenized_text = unbias.tokenize_for_prediction('The man was hungry.', tokenizer)\nprint(tokenized_text)\n</code></pre> <p> Output  <code>[ints representing token ids], [0s and 1s]</code></p>"}]}